{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Training\n",
    "\n",
    "Uses the Trainer included in Hugging Face `transformers` (backed by `accelerate`) since it mitigates a lot of annoying boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxwoolf/Library/Mobile Documents/com~apple~CloudDocs/PythonProjects/imdb-embeddings/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import Trainer, TrainingArguments, ModernBertConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10_000, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tconst</th><th>averageRating</th><th>json</th></tr><tr><td>str</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>&quot;tt0000009&quot;</td><td>5.4</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Miss Jerry&quot;,\n",
       "&nbsp;&nbsp;&quot;…</td></tr><tr><td>&quot;tt0000147&quot;</td><td>5.3</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Corbett-Fitz…</td></tr><tr><td>&quot;tt0000574&quot;</td><td>6.0</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Story of the…</td></tr><tr><td>&quot;tt0000591&quot;</td><td>5.6</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Prodigal Son…</td></tr><tr><td>&quot;tt0000630&quot;</td><td>3.2</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Hamlet&quot;,\n",
       "&nbsp;&nbsp;&quot;genr…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;tt0035474&quot;</td><td>6.9</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;True to the Army…</td></tr><tr><td>&quot;tt0035475&quot;</td><td>6.6</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Trysil-Knut&quot;,\n",
       "&nbsp;&nbsp;…</td></tr><tr><td>&quot;tt0035477&quot;</td><td>6.0</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Tuttles of T…</td></tr><tr><td>&quot;tt0035479&quot;</td><td>5.4</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Twin Beds&quot;,\n",
       "&nbsp;&nbsp;&quot;g…</td></tr><tr><td>&quot;tt0035480&quot;</td><td>5.6</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Two Weeks to Liv…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10_000, 3)\n",
       "┌───────────┬───────────────┬───────────────────────────────┐\n",
       "│ tconst    ┆ averageRating ┆ json                          │\n",
       "│ ---       ┆ ---           ┆ ---                           │\n",
       "│ str       ┆ f32           ┆ str                           │\n",
       "╞═══════════╪═══════════════╪═══════════════════════════════╡\n",
       "│ tt0000009 ┆ 5.4           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"Miss Jerry\",      │\n",
       "│           ┆               ┆   \"…                          │\n",
       "│ tt0000147 ┆ 5.3           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"The Corbett-Fitz… │\n",
       "│ tt0000574 ┆ 6.0           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"The Story of the… │\n",
       "│ tt0000591 ┆ 5.6           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"The Prodigal Son… │\n",
       "│ tt0000630 ┆ 3.2           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"Hamlet\",          │\n",
       "│           ┆               ┆   \"genr…                      │\n",
       "│ …         ┆ …             ┆ …                             │\n",
       "│ tt0035474 ┆ 6.9           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"True to the Army… │\n",
       "│ tt0035475 ┆ 6.6           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"Trysil-Knut\",     │\n",
       "│           ┆               ┆   …                           │\n",
       "│ tt0035477 ┆ 6.0           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"The Tuttles of T… │\n",
       "│ tt0035479 ┆ 5.4           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"Twin Beds\",       │\n",
       "│           ┆               ┆   \"g…                         │\n",
       "│ tt0035480 ┆ 5.6           ┆ {                             │\n",
       "│           ┆               ┆   \"title\": \"Two Weeks to Liv… │\n",
       "└───────────┴───────────────┴───────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pl.scan_parquet(\n",
    "        \"/Users/maxwoolf/Downloads/movie_data_plus_embeds_all.parquet\", n_rows=10000\n",
    "    )\n",
    "    .select([\"tconst\", \"averageRating\", \"json\"])\n",
    "    .with_columns(averageRating=pl.col(\"averageRating\").cast(pl.Float32))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Custom Tokenizer\n",
    "\n",
    "Use uses the `modernbert` tokenizer as a base (since it has useful special tokens), just reduce the vocabulary significantly and tailor it to this specific dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kets': 43846, 'áĢº': 33160, 'ĠDowntown': 46827, 're': 250, 'ĠTan': 22188, 'Ġsinus': 22749, 'Ð¸Ñħ':\n",
      "169\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "json_docs = df[\"json\"].to_list()\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "print(str(base_tokenizer.vocab)[0:100])\n",
    "print(len(base_tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{'ĠPearson': 4183, 'ĠEm': 1047, 'ĠKendall': 3814, 'oll': 910, 'Isabel': 2855, 'ĠRawlinson': 3588, 'b\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    iter(json_docs), vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "print(str(tokenizer.vocab)[0:100])\n",
    "print(len(tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preencode all the tokens. Set max length to `1024` to be safe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 1024\n",
    "\n",
    "tokens = tokenizer(json_docs, max_length=max_length, padding=\"max_length\")\n",
    "len(tokens[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lengths = [len(x) for x in tokens[\"input_ids\"]]\n",
    "max(input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "tensor_input_ids = torch.Tensor(tokens[\"input_ids\"]).int().to(device)\n",
    "tensor_attention_mask = torch.Tensor(tokens[\"attention_mask\"]).int().to(device)\n",
    "tensor_ratings = torch.from_numpy(df[\"averageRating\"].to_numpy().copy()).to(device)\n",
    "tensor_dataset = TensorDataset(tensor_input_ids, tensor_attention_mask, tensor_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion = 0.05\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tensor_dataset, [1 - test_proportion, test_proportion]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Due to the new tokenizer, the special tokens for the fresh ModernBERT model have to be explicitly defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': 2,\n",
       " 'sep_token': 4,\n",
       " 'pad_token': 5,\n",
       " 'cls_token': 3,\n",
       " 'mask_token': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token_dict = dict(\n",
    "    zip(tokenizer.special_tokens_map.keys(), tokenizer.all_special_ids)\n",
    ")\n",
    "special_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8100096"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ModernBertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_length,\n",
    "    embedding_size=256,\n",
    "    hidden_size=256,\n",
    "    intermediate_size=768,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=4,\n",
    "    unk_token_id=special_token_dict[\"unk_token\"],\n",
    "    sep_token_id=special_token_dict[\"sep_token\"],\n",
    "    pad_token_id=special_token_dict[\"pad_token\"],\n",
    "    cls_token_id=special_token_dict[\"cls_token\"],\n",
    "    mask_token_id=special_token_dict[\"mask_token\"],\n",
    ")\n",
    "\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.transformer_model = model\n",
    "        self.output = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, targets=None):\n",
    "        x = self.transformer_model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        x = x.last_hidden_state[:, 0]  # the [CLS] vector\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x.squeeze()  # return 1D output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RatingsModel(\n",
       "  (transformer_model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(5000, 256, padding_idx=5)\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=256, out_features=1536, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=256, out_features=1536, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss doesn't play nice with the `Trainer` out of the boss, so need [some tweaks](https://discuss.huggingface.co/t/no-log-for-validation-loss-during-training-with-trainer/40094/3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids = torch.stack([f[0] for f in examples])\n",
    "    attention_masks = torch.stack([f[1] for f in examples])\n",
    "    targets = torch.stack([f[2] for f in examples])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"targets\": targets,\n",
    "    }\n",
    "\n",
    "\n",
    "class MAETrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=0):\n",
    "        outputs = model(**inputs)\n",
    "        loss = nn.L1Loss()(outputs, inputs[\"targets\"])  # L1 loss is MAE\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-3,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.001,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.05,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,  # since data is in memory\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=False,\n",
    ")\n",
    "\n",
    "# reinstantiate a clean model\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "trainer = MAETrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2375' max='2375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2375/2375 09:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.731500</td>\n",
       "      <td>0.652724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.643700</td>\n",
       "      <td>0.658383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>0.694192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.687195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>0.663770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.657295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.656480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>0.650622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.653892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.665260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>0.648998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1428</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.648058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1547</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.647854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>0.655700</td>\n",
       "      <td>0.645254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.650050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1904</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.645317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>0.583300</td>\n",
       "      <td>0.638655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2142</td>\n",
       "      <td>0.591100</td>\n",
       "      <td>0.635252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2261</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.634814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2375, training_loss=0.6285669041683799, metrics={'train_runtime': 597.1282, 'train_samples_per_second': 15.909, 'train_steps_per_second': 3.977, 'total_flos': 0.0, 'train_loss': 0.6285669041683799, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6347675919532776,\n",
       " 'eval_runtime': 7.2476,\n",
       " 'eval_samples_per_second': 68.988,\n",
       " 'eval_steps_per_second': 17.247,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Predicted</th><th>Actual</th><th>abs_diff</th></tr><tr><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>6.495729</td><td>6.7</td><td>0.2</td></tr><tr><td>6.369858</td><td>6.7</td><td>0.33</td></tr><tr><td>6.074883</td><td>7.1</td><td>1.03</td></tr><tr><td>6.30686</td><td>8.1</td><td>1.79</td></tr><tr><td>6.118845</td><td>6.0</td><td>0.12</td></tr><tr><td>6.196302</td><td>5.1</td><td>1.1</td></tr><tr><td>5.905068</td><td>5.6</td><td>0.31</td></tr><tr><td>6.30401</td><td>5.4</td><td>0.9</td></tr><tr><td>6.25199</td><td>5.7</td><td>0.55</td></tr><tr><td>6.122697</td><td>5.8</td><td>0.32</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 3)\n",
       "┌───────────┬────────┬──────────┐\n",
       "│ Predicted ┆ Actual ┆ abs_diff │\n",
       "│ ---       ┆ ---    ┆ ---      │\n",
       "│ f32       ┆ f32    ┆ f32      │\n",
       "╞═══════════╪════════╪══════════╡\n",
       "│ 6.495729  ┆ 6.7    ┆ 0.2      │\n",
       "│ 6.369858  ┆ 6.7    ┆ 0.33     │\n",
       "│ 6.074883  ┆ 7.1    ┆ 1.03     │\n",
       "│ 6.30686   ┆ 8.1    ┆ 1.79     │\n",
       "│ 6.118845  ┆ 6.0    ┆ 0.12     │\n",
       "│ 6.196302  ┆ 5.1    ┆ 1.1      │\n",
       "│ 5.905068  ┆ 5.6    ┆ 0.31     │\n",
       "│ 6.30401   ┆ 5.4    ┆ 0.9      │\n",
       "│ 6.25199   ┆ 5.7    ┆ 0.55     │\n",
       "│ 6.122697  ┆ 5.8    ┆ 0.32     │\n",
       "└───────────┴────────┴──────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = model.to(device)\n",
    "eval_dataset = test_dataset[0:10]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=eval_dataset[0], attention_mask=eval_dataset[1])\n",
    "    preds = output.detach().cpu()\n",
    "\n",
    "pl.DataFrame({\"Predicted\": preds, \"Actual\": eval_dataset[2]}).with_columns(\n",
    "    abs_diff=(pl.col(\"Predicted\") - pl.col(\"Actual\")).abs().round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
