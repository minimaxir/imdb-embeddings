{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Training\n",
    "\n",
    "Uses the Trainer included in Hugging Face `transformers` (backed by `accelerate`) since it mitigates a lot of annoying boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import Trainer, TrainingArguments, ModernBertConfig, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (242_552, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tconst</th><th>averageRating</th><th>json</th></tr><tr><td>str</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>&quot;tt0173052&quot;</td><td>4.1</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Prince and t…</td></tr><tr><td>&quot;tt0266288&quot;</td><td>7.4</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Azhakiya Ravanan…</td></tr><tr><td>&quot;tt6263490&quot;</td><td>4.3</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Getaway&quot;,\n",
       "&nbsp;&nbsp;&quot;gen…</td></tr><tr><td>&quot;tt10049110&quot;</td><td>7.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Die Wiese&quot;,\n",
       "&nbsp;&nbsp;&quot;g…</td></tr><tr><td>&quot;tt5761612&quot;</td><td>3.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Woman on the Edg…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;tt0079376&quot;</td><td>6.2</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Proud Twins&quot;…</td></tr><tr><td>&quot;tt1161064&quot;</td><td>3.2</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Super Capers: Th…</td></tr><tr><td>&quot;tt0179526&quot;</td><td>5.7</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Who&#x27;s the Caboos…</td></tr><tr><td>&quot;tt0188233&quot;</td><td>5.7</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;That&#x27;s Erotic&quot;,\n",
       "…</td></tr><tr><td>&quot;tt0082518&quot;</td><td>5.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Hoge hakken, ech…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (242_552, 3)\n",
       "┌────────────┬───────────────┬───────────────────────────────┐\n",
       "│ tconst     ┆ averageRating ┆ json                          │\n",
       "│ ---        ┆ ---           ┆ ---                           │\n",
       "│ str        ┆ f32           ┆ str                           │\n",
       "╞════════════╪═══════════════╪═══════════════════════════════╡\n",
       "│ tt0173052  ┆ 4.1           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"The Prince and t… │\n",
       "│ tt0266288  ┆ 7.4           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Azhakiya Ravanan… │\n",
       "│ tt6263490  ┆ 4.3           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Getaway\",         │\n",
       "│            ┆               ┆   \"gen…                       │\n",
       "│ tt10049110 ┆ 7.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Die Wiese\",       │\n",
       "│            ┆               ┆   \"g…                         │\n",
       "│ tt5761612  ┆ 3.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Woman on the Edg… │\n",
       "│ …          ┆ …             ┆ …                             │\n",
       "│ tt0079376  ┆ 6.2           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"The Proud Twins\"… │\n",
       "│ tt1161064  ┆ 3.2           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Super Capers: Th… │\n",
       "│ tt0179526  ┆ 5.7           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Who's the Caboos… │\n",
       "│ tt0188233  ┆ 5.7           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"That's Erotic\",   │\n",
       "│            ┆               ┆ …                             │\n",
       "│ tt0082518  ┆ 5.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Hoge hakken, ech… │\n",
       "└────────────┴───────────────┴───────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pl.scan_parquet(\n",
    "        \"movie_data_plus_embeds_all.parquet\",\n",
    "    )\n",
    "    .select([\"tconst\", \"averageRating\", \"json\"])\n",
    "    .with_columns(averageRating=pl.col(\"averageRating\").cast(pl.Float32))\n",
    "    .collect()\n",
    "    .sample(fraction=1.0, shuffle=True, seed=42)\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Custom Tokenizer\n",
    "\n",
    "Use the `modernbert` tokenizer as a base, just reduce the vocabulary significantly and tailor it to this specific dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'devices': 49242, 'ĠLloyd': 24395, 'ĠR': 416, 'ĠTruck': 42712, 'Ġsatisfies': 12310, 'Follow': 18905\n",
      "378\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "json_docs = df[\"json\"].to_list()\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "print(str(base_tokenizer.vocab)[0:100])\n",
    "print(len(base_tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{'lick': 4092, 'yl': 1128, 'ĠAp': 2639, 'ď': 210, 'mith': 856, 'ò': 181, 'Rand': 3915, 'Horror': 665\n",
      "302\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "# don't train on all texts because it will take forever\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    iter(json_docs[:50000]), vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "print(str(tokenizer.vocab)[0:100])\n",
    "print(len(tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preencode all the tokens. A `max_length` of 1024 may be excessive but does not cause a proportionate reduction in model training speed over a 512 max length due to ModernBERT's unpadding.\n",
    "\n",
    "In order to avoid OOMs on the host system, generate in batches, then push to the GPU. (ideally we _could_ push to the GPU for each batch but that will cause GPU memory leaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [03:21,  1.70s/it]                         \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([242552, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 1024\n",
    "token_batch_size = 2048\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# input_ids = torch.empty((0, max_length)).to(\"cpu\")\n",
    "# attention_mask = torch.empty((0, max_length)).to(\"cpu\")\n",
    "\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for docs in tqdm(batch(json_docs, token_batch_size), total=len(json_docs) // token_batch_size):\n",
    "    tokens = tokenizer(docs,\n",
    "                       max_length=max_length,\n",
    "                       padding=\"max_length\",\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\").to(\"cpu\")\n",
    "    \n",
    "    # input_ids = torch.vstack([input_ids, tokens[\"input_ids\"]])\n",
    "    # attention_mask = torch.vstack([attention_mask, tokens[\"attention_mask\"]])\n",
    "    \n",
    "    input_ids.append(tokens[\"input_ids\"])\n",
    "    attention_mask.append(tokens[\"attention_mask\"])\n",
    "   \n",
    "input_ids = torch.vstack(input_ids).to(device)\n",
    "attention_mask = torch.vstack(attention_mask).to(device)\n",
    "\n",
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1000, 7.4000, 4.3000,  ..., 6.4000, 6.0000, 6.5000], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "n_test = 20000\n",
    "\n",
    "X_input_ids_train = input_ids[:-n_test].int().to(device)\n",
    "X_input_ids_test = input_ids[-n_test:].int().to(device)\n",
    "\n",
    "X_attention_train = attention_mask[:-n_test].int().to(device)\n",
    "X_attention_test = attention_mask[-n_test:].int().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(df[:-n_test][\"averageRating\"].to_numpy().copy()).to(device)\n",
    "y_test = torch.from_numpy(df[-n_test:][\"averageRating\"].to_numpy().copy()).to(device)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_input_ids_train, X_attention_train, y_train)\n",
    "test_dataset = TensorDataset(X_input_ids_test, X_attention_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Due to the new tokenizer, the special tokens for the fresh ModernBERT model have to be explicitly defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': 2,\n",
       " 'sep_token': 4,\n",
       " 'pad_token': 5,\n",
       " 'cls_token': 3,\n",
       " 'mask_token': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token_dict = dict(\n",
    "    zip(tokenizer.special_tokens_map.keys(), tokenizer.all_special_ids)\n",
    ")\n",
    "special_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4968576"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "config = ModernBertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_length,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=8,\n",
    "    unk_token_id=special_token_dict[\"unk_token\"],\n",
    "    sep_token_id=special_token_dict[\"sep_token\"],\n",
    "    pad_token_id=special_token_dict[\"pad_token\"],\n",
    "    cls_token_id=special_token_dict[\"cls_token\"],\n",
    "    mask_token_id=special_token_dict[\"mask_token\"],\n",
    ")\n",
    "\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RatingsModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.transformer_model = model\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, targets=None):\n",
    "        x = self.transformer_model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        x = x.last_hidden_state[:, 0]  # the [CLS] vector\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x.squeeze()  # return 1D output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # perf increase for ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss doesn't play nice with the `Trainer` out of the boss, so need [some tweaks](https://discuss.huggingface.co/t/no-log-for-validation-loss-during-training-with-trainer/40094/3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids = torch.stack([f[0] for f in examples])\n",
    "    attention_masks = torch.stack([f[1] for f in examples])\n",
    "    targets = torch.stack([f[2] for f in examples])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"targets\": targets,\n",
    "    }\n",
    "\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=0):\n",
    "        outputs = model(**inputs)\n",
    "        # loss = nn.L1Loss()(outputs, inputs[\"targets\"])  # L1 loss is MAE\n",
    "        loss = nn.MSELoss()(outputs, inputs[\"targets\"])\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-3,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.001,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.05,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=0,  # since data is in memory, as problem is GPU bound\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=False,\n",
    ")\n",
    "\n",
    "# reinstantiate a clean model\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "trainer = RegressionTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 01:47:29.129000 15552 site-packages/torch/_inductor/utils.py:1250] [1/4] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34780' max='34780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34780/34780 31:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1739</td>\n",
       "      <td>1.356700</td>\n",
       "      <td>1.147596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3478</td>\n",
       "      <td>1.212400</td>\n",
       "      <td>1.131216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5217</td>\n",
       "      <td>1.152500</td>\n",
       "      <td>1.124591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6956</td>\n",
       "      <td>1.150600</td>\n",
       "      <td>1.105348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8695</td>\n",
       "      <td>1.117200</td>\n",
       "      <td>1.090652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10434</td>\n",
       "      <td>1.101900</td>\n",
       "      <td>1.113789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12173</td>\n",
       "      <td>1.061200</td>\n",
       "      <td>1.093917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13912</td>\n",
       "      <td>1.071700</td>\n",
       "      <td>1.058538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15651</td>\n",
       "      <td>1.012700</td>\n",
       "      <td>1.068710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17390</td>\n",
       "      <td>1.025500</td>\n",
       "      <td>1.053821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19129</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.072554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20868</td>\n",
       "      <td>0.948600</td>\n",
       "      <td>1.053987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22607</td>\n",
       "      <td>0.865900</td>\n",
       "      <td>1.101942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24346</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>1.086910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26085</td>\n",
       "      <td>0.757400</td>\n",
       "      <td>1.148504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27824</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>1.138774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29563</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>1.193457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31302</td>\n",
       "      <td>0.661900</td>\n",
       "      <td>1.197769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33041</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>1.220957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34780</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>1.225109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34780, training_loss=0.9480032518857371, metrics={'train_runtime': 1883.1121, 'train_samples_per_second': 1181.831, 'train_steps_per_second': 18.469, 'total_flos': 0.0, 'train_loss': 0.9480032518857371, 'epoch': 10.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2251088619232178,\n",
       " 'eval_runtime': 5.1475,\n",
       " 'eval_samples_per_second': 3885.398,\n",
       " 'eval_steps_per_second': 60.806,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.3567,\n",
       "  'grad_norm': 7.730082035064697,\n",
       "  'learning_rate': 0.0009938864888000992,\n",
       "  'epoch': 0.5,\n",
       "  'step': 1739},\n",
       " {'eval_loss': 1.1475961208343506,\n",
       "  'eval_runtime': 8.6645,\n",
       "  'eval_samples_per_second': 2308.258,\n",
       "  'eval_steps_per_second': 36.124,\n",
       "  'epoch': 0.5,\n",
       "  'step': 1739},\n",
       " {'loss': 1.2124,\n",
       "  'grad_norm': 11.11757755279541,\n",
       "  'learning_rate': 0.0009756119265622743,\n",
       "  'epoch': 1.0,\n",
       "  'step': 3478},\n",
       " {'eval_loss': 1.1312155723571777,\n",
       "  'eval_runtime': 5.213,\n",
       "  'eval_samples_per_second': 3836.568,\n",
       "  'eval_steps_per_second': 60.042,\n",
       "  'epoch': 1.0,\n",
       "  'step': 3478}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = trainer.state.log_history\n",
    "\n",
    "logs[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (21, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>loss</th><th>grad_norm</th><th>learning_rate</th><th>epoch</th><th>step</th><th>eval_loss</th><th>eval_runtime</th><th>eval_samples_per_second</th><th>eval_steps_per_second</th><th>train_runtime</th><th>train_samples_per_second</th><th>train_steps_per_second</th><th>total_flos</th><th>train_loss</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.3567</td><td>7.730082</td><td>0.000994</td><td>0.5</td><td>1739</td><td>1.147596</td><td>8.6645</td><td>2308.258</td><td>36.124</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1.2124</td><td>11.117578</td><td>0.000976</td><td>1.0</td><td>3478</td><td>1.131216</td><td>5.213</td><td>3836.568</td><td>60.042</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1.1525</td><td>3.543578</td><td>0.000946</td><td>1.5</td><td>5217</td><td>1.124591</td><td>5.2206</td><td>3830.961</td><td>59.955</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1.1506</td><td>7.620698</td><td>0.000905</td><td>2.0</td><td>6956</td><td>1.105348</td><td>5.1827</td><td>3858.967</td><td>60.393</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1.1172</td><td>4.802451</td><td>0.000854</td><td>2.5</td><td>8695</td><td>1.090652</td><td>5.2121</td><td>3837.231</td><td>60.053</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.665</td><td>1.33698</td><td>0.000055</td><td>8.5</td><td>29563</td><td>1.193457</td><td>5.1953</td><td>3849.634</td><td>60.247</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.6619</td><td>5.775957</td><td>0.000025</td><td>9.0</td><td>31302</td><td>1.197769</td><td>5.182</td><td>3859.506</td><td>60.401</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.6089</td><td>4.370686</td><td>0.000006</td><td>9.5</td><td>33041</td><td>1.220957</td><td>5.1857</td><td>3856.752</td><td>60.358</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.6102</td><td>7.304399</td><td>6.6088e-10</td><td>10.0</td><td>34780</td><td>1.225109</td><td>5.1692</td><td>3869.081</td><td>60.551</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>10.0</td><td>34780</td><td>1.225109</td><td>5.1475</td><td>3885.398</td><td>60.806</td><td>1883.1121</td><td>1181.831</td><td>18.469</td><td>0.0</td><td>0.948003</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (21, 14)\n",
       "┌────────┬───────────┬─────────────┬───────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ loss   ┆ grad_norm ┆ learning_ra ┆ epoch ┆ … ┆ train_samp ┆ train_step ┆ total_flos ┆ train_loss │\n",
       "│ ---    ┆ ---       ┆ te          ┆ ---   ┆   ┆ les_per_se ┆ s_per_seco ┆ ---        ┆ ---        │\n",
       "│ f64    ┆ f64       ┆ ---         ┆ f64   ┆   ┆ cond       ┆ nd         ┆ f64        ┆ f64        │\n",
       "│        ┆           ┆ f64         ┆       ┆   ┆ ---        ┆ ---        ┆            ┆            │\n",
       "│        ┆           ┆             ┆       ┆   ┆ f64        ┆ f64        ┆            ┆            │\n",
       "╞════════╪═══════════╪═════════════╪═══════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 1.3567 ┆ 7.730082  ┆ 0.000994    ┆ 0.5   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ 1.2124 ┆ 11.117578 ┆ 0.000976    ┆ 1.0   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ 1.1525 ┆ 3.543578  ┆ 0.000946    ┆ 1.5   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ 1.1506 ┆ 7.620698  ┆ 0.000905    ┆ 2.0   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ 1.1172 ┆ 4.802451  ┆ 0.000854    ┆ 2.5   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ …      ┆ …         ┆ …           ┆ …     ┆ … ┆ …          ┆ …          ┆ …          ┆ …          │\n",
       "│ 0.665  ┆ 1.33698   ┆ 0.000055    ┆ 8.5   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ 0.6619 ┆ 5.775957  ┆ 0.000025    ┆ 9.0   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ 0.6089 ┆ 4.370686  ┆ 0.000006    ┆ 9.5   ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ 0.6102 ┆ 7.304399  ┆ 6.6088e-10  ┆ 10.0  ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
       "│ null   ┆ null      ┆ null        ┆ 10.0  ┆ … ┆ 1181.831   ┆ 18.469     ┆ 0.0        ┆ 0.948003   │\n",
       "└────────┴───────────┴─────────────┴───────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_consolidated = []\n",
    "i = 0\n",
    "while i < len(logs)-1:\n",
    "    base_log = logs[i]\n",
    "    base_log.update(logs[i+1])\n",
    "    logs_consolidated.append(base_log)\n",
    "    i += 2\n",
    "    \n",
    "df_logs = pl.DataFrame(logs_consolidated).sort(\"epoch\")\n",
    "df_logs.write_parquet(\"llm_scratch_train_logs.parquet\")\n",
    "df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_model\n",
    "\n",
    "save_model(model, \"imdb_embeddings_llm_scratch.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.50 GiB. GPU 0 has a total capacity of 21.95 GiB of which 6.93 GiB is free. Including non-PyTorch memory, this process has 15.01 GiB memory in use. Of the allocated memory 13.40 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m actual_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([f[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m test_dataset])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_attention_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     preds \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     12\u001b[0m test_results \u001b[38;5;241m=\u001b[39m (pl\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m: preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m\"\u001b[39m: actual_values\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()})\n\u001b[1;32m     13\u001b[0m                 \u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[1;32m     14\u001b[0m                     abs_diff\u001b[38;5;241m=\u001b[39m(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mabs(),\n\u001b[1;32m     15\u001b[0m                     square_error \u001b[38;5;241m=\u001b[39m ((pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     16\u001b[0m                 )\n\u001b[1;32m     17\u001b[0m                )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:814\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:802\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mRatingsModel.forward\u001b[0;34m(self, input_ids, attention_mask, targets)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# the [CLS] vector\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py:962\u001b[0m, in \u001b[0;36mModernBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    951\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    952\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    953\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    959\u001b[0m         output_attentions,\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 962\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py:556\u001b[0m, in \u001b[0;36mModernBertEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, sliding_window_mask, position_ids, cu_seqlens, max_seqlen, output_attentions)\u001b[0m\n\u001b[1;32m    545\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_norm(hidden_states),\n\u001b[1;32m    547\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    555\u001b[0m mlp_output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mreference_compile\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_norm(hidden_states))\n\u001b[1;32m    559\u001b[0m )\n\u001b[1;32m    560\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m mlp_output\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (hidden_states,) \u001b[38;5;241m+\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py:531\u001b[0m, in \u001b[0;36mModernBertEncoderLayer.compiled_mlp\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnorm_eps, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnorm_bias)\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m ModernBertMLP(config)\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompiled_mlp\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_norm(hidden_states))\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1201\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1199\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1200\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:328\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[1;32m    327\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 328\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:689\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    686\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    687\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 689\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:495\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    488\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    489\u001b[0m         runtime_metadata,\n\u001b[1;32m    490\u001b[0m         out,\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_inductor/output_code.py:460\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     get_runtime_metrics_context()\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_inductor/utils.py:2404\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(new_inputs: \u001b[38;5;28mlist\u001b[39m[InputType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   2403\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m-> 2404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/tmp/torchinductor_jupyter/42/c423oz6wcsvw2wxoa3kxbk3h2rfnr5htnuluvyc326jymf7t7adx.py:258\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    256\u001b[0m triton_poi_fused__to_copy_1\u001b[38;5;241m.\u001b[39mrun(arg3_1, buf4, \u001b[38;5;241m196608\u001b[39m, stream\u001b[38;5;241m=\u001b[39mstream0)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arg3_1\n\u001b[0;32m--> 258\u001b[0m buf5 \u001b[38;5;241m=\u001b[39m \u001b[43mempty_strided_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1536\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1536\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [layer_norm, linear], Original ATen: [aten.native_layer_norm, aten._to_copy, aten.mm]\u001b[39;00m\n\u001b[1;32m    260\u001b[0m extern_kernels\u001b[38;5;241m.\u001b[39mmm(buf3, reinterpret_tensor(buf4, (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1536\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m), \u001b[38;5;241m0\u001b[39m), out\u001b[38;5;241m=\u001b[39mbuf5)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.50 GiB. GPU 0 has a total capacity of 21.95 GiB of which 6.93 GiB is free. Including non-PyTorch memory, this process has 15.01 GiB memory in use. Of the allocated memory 13.40 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "_ = model.to(\"cuda:0\").eval()  # to disable BatchNorm1D\n",
    "\n",
    "eval_input_ids = torch.stack([f[0] for f in test_dataset])\n",
    "eval_attention_masks = torch.stack([f[1] for f in test_dataset])\n",
    "actual_values = torch.stack([f[2] for f in test_dataset])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=eval_input_ids.to(\"cuda:0\"),\n",
    "                   attention_mask=eval_attention_masks.to(\"cuda:0\"))\n",
    "    preds = output.detach().cpu().numpy()\n",
    "\n",
    "test_results = (pl.DataFrame({\"Predicted\": preds, \"Actual\": actual_values.cpu().numpy()})\n",
    "                .with_columns(\n",
    "                    abs_diff=(pl.col(\"Predicted\") - pl.col(\"Actual\")).abs(),\n",
    "                    square_error = ((pl.col(\"Actual\") - pl.col(\"Predicted\")) ** 2)\n",
    "                )\n",
    "               )\n",
    "                \n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "test_results[\"abs_diff\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Square Error (MSE)\n",
    "test_results[\"square_error\"].mean()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu123.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu123:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
