{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Training\n",
    "\n",
    "Uses the Trainer included in Hugging Face `transformers` (backed by `accelerate`) since it mitigates a lot of annoying boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import Trainer, TrainingArguments, ModernBertConfig, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (242_552, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tconst</th><th>averageRating</th><th>json</th></tr><tr><td>str</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>&quot;tt0173052&quot;</td><td>4.1</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Prince and t…</td></tr><tr><td>&quot;tt0266288&quot;</td><td>7.4</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Azhakiya Ravanan…</td></tr><tr><td>&quot;tt6263490&quot;</td><td>4.3</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Getaway&quot;,\n",
       "&nbsp;&nbsp;&quot;gen…</td></tr><tr><td>&quot;tt10049110&quot;</td><td>7.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Die Wiese&quot;,\n",
       "&nbsp;&nbsp;&quot;g…</td></tr><tr><td>&quot;tt5761612&quot;</td><td>3.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Woman on the Edg…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;tt0079376&quot;</td><td>6.2</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Proud Twins&quot;…</td></tr><tr><td>&quot;tt1161064&quot;</td><td>3.2</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Super Capers: Th…</td></tr><tr><td>&quot;tt0179526&quot;</td><td>5.7</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Who&#x27;s the Caboos…</td></tr><tr><td>&quot;tt0188233&quot;</td><td>5.7</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;That&#x27;s Erotic&quot;,\n",
       "…</td></tr><tr><td>&quot;tt0082518&quot;</td><td>5.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Hoge hakken, ech…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (242_552, 3)\n",
       "┌────────────┬───────────────┬───────────────────────────────┐\n",
       "│ tconst     ┆ averageRating ┆ json                          │\n",
       "│ ---        ┆ ---           ┆ ---                           │\n",
       "│ str        ┆ f32           ┆ str                           │\n",
       "╞════════════╪═══════════════╪═══════════════════════════════╡\n",
       "│ tt0173052  ┆ 4.1           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"The Prince and t… │\n",
       "│ tt0266288  ┆ 7.4           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Azhakiya Ravanan… │\n",
       "│ tt6263490  ┆ 4.3           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Getaway\",         │\n",
       "│            ┆               ┆   \"gen…                       │\n",
       "│ tt10049110 ┆ 7.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Die Wiese\",       │\n",
       "│            ┆               ┆   \"g…                         │\n",
       "│ tt5761612  ┆ 3.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Woman on the Edg… │\n",
       "│ …          ┆ …             ┆ …                             │\n",
       "│ tt0079376  ┆ 6.2           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"The Proud Twins\"… │\n",
       "│ tt1161064  ┆ 3.2           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Super Capers: Th… │\n",
       "│ tt0179526  ┆ 5.7           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Who's the Caboos… │\n",
       "│ tt0188233  ┆ 5.7           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"That's Erotic\",   │\n",
       "│            ┆               ┆ …                             │\n",
       "│ tt0082518  ┆ 5.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Hoge hakken, ech… │\n",
       "└────────────┴───────────────┴───────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pl.scan_parquet(\n",
    "        \"movie_data_plus_embeds_all.parquet\",\n",
    "    )\n",
    "    .select([\"tconst\", \"averageRating\", \"json\"])\n",
    "    .with_columns(averageRating=pl.col(\"averageRating\").cast(pl.Float32))\n",
    "    .collect()\n",
    "    .sample(fraction=1.0, shuffle=True, seed=42)\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Custom Tokenizer\n",
    "\n",
    "Use the `modernbert` tokenizer as a base, just reduce the vocabulary significantly and tailor it to this specific dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ġbanned': 20374, 'Ġexisting': 5368, 'Ġadmissions': 26120, 'lund': 45815, 'Ġ\\\\,\\\\': 28247, 'Ġfav': \n",
      "378\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "json_docs = df[\"json\"].to_list()\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "print(str(base_tokenizer.vocab)[0:100])\n",
    "print(len(base_tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{'Erik': 2006, 'ĠNguyen': 4954, 'tti': 1325, 'ifer': 1282, 'ler': 619, 'rass': 4854, 'Ã´me': 4557, '\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "# don't train on all texts because it will take forever\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    iter(json_docs[:50000]), vocab_size=vocab_size,\n",
    "    new_special_tokens=[\"  \", \"    \", \"      \"]\n",
    ")\n",
    "\n",
    "print(str(tokenizer.vocab)[0:100])\n",
    "print(len(tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preencode all the tokens. A `max_length` of 1024 may be excessive but does not cause a proportionate reduction in model training speed over a 512 max length due to ModernBERT's unpadding + RoPE.\n",
    "\n",
    "In order to avoid OOMs on the host system, generate in batches, then push to the GPU. (ideally we _could_ push to the GPU for each batch but that will cause GPU memory leaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [03:38,  1.83s/it]                         \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([242552, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 1024\n",
    "token_batch_size = 2048\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# input_ids = torch.empty((0, max_length)).to(\"cpu\")\n",
    "# attention_mask = torch.empty((0, max_length)).to(\"cpu\")\n",
    "\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for docs in tqdm(batch(json_docs, token_batch_size), total=len(json_docs) // token_batch_size):\n",
    "    tokens = tokenizer(docs,\n",
    "                       max_length=max_length,\n",
    "                       padding=\"max_length\",\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\").to(\"cpu\")\n",
    "    \n",
    "    # input_ids = torch.vstack([input_ids, tokens[\"input_ids\"]])\n",
    "    # attention_mask = torch.vstack([attention_mask, tokens[\"attention_mask\"]])\n",
    "    \n",
    "    input_ids.append(tokens[\"input_ids\"])\n",
    "    attention_mask.append(tokens[\"attention_mask\"])\n",
    "   \n",
    "input_ids = torch.vstack(input_ids).to(device)\n",
    "attention_mask = torch.vstack(attention_mask).to(device)\n",
    "\n",
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3, 100, 208,   7,  11, 359, 271, 267, 525, 377], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token id 7 is the 2-space token for indents\n",
    "input_ids[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1000, 7.4000, 4.3000,  ..., 6.4000, 6.0000, 6.5000], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "n_test = 20000\n",
    "\n",
    "X_input_ids_train = input_ids[:-n_test].int().to(device)\n",
    "X_input_ids_test = input_ids[-n_test:].int().to(device)\n",
    "\n",
    "X_attention_train = attention_mask[:-n_test].int().to(device)\n",
    "X_attention_test = attention_mask[-n_test:].int().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(df[:-n_test][\"averageRating\"].to_numpy().copy()).to(device)\n",
    "y_test = torch.from_numpy(df[-n_test:][\"averageRating\"].to_numpy().copy()).to(device)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_input_ids_train, X_attention_train, y_train)\n",
    "test_dataset = TensorDataset(X_input_ids_test, X_attention_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Due to the new tokenizer, the special tokens for the fresh ModernBERT model have to be explicitly defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': 2,\n",
       " 'sep_token': 4,\n",
       " 'pad_token': 5,\n",
       " 'cls_token': 3,\n",
       " 'mask_token': 6,\n",
       " 'additional_special_tokens': 7}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token_dict = dict(\n",
    "    zip(tokenizer.special_tokens_map.keys(), tokenizer.all_special_ids)\n",
    ")\n",
    "special_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2214528"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "dropout = 0.5\n",
    "\n",
    "config = ModernBertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_length,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=512,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    global_attn_every_n_layers=2,\n",
    "    local_attention=16,\n",
    "    attention_dropout=dropout,\n",
    "    embeddings_dropout=dropout,\n",
    "    mlp_dropout=dropout,\n",
    "    unk_token_id=special_token_dict[\"unk_token\"],\n",
    "    sep_token_id=special_token_dict[\"sep_token\"],\n",
    "    pad_token_id=special_token_dict[\"pad_token\"],\n",
    "    cls_token_id=special_token_dict[\"cls_token\"],\n",
    "    mask_token_id=special_token_dict[\"mask_token\"],\n",
    ")\n",
    "\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RatingsModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.transformer_model = model\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, targets=None):\n",
    "        x = self.transformer_model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        x = x.last_hidden_state[:, 0]  # the [CLS] vector\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x.squeeze()  # return 1D output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # perf increase for ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss doesn't play nice with the `Trainer` out of the boss, so need [some tweaks](https://discuss.huggingface.co/t/no-log-for-validation-loss-during-training-with-trainer/40094/3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids = torch.stack([f[0] for f in examples])\n",
    "    attention_masks = torch.stack([f[1] for f in examples])\n",
    "    targets = torch.stack([f[2] for f in examples])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"targets\": targets,\n",
    "    }\n",
    "\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=0):\n",
    "        outputs = model(**inputs)\n",
    "        # loss = nn.L1Loss()(outputs, inputs[\"targets\"])  # L1 loss is MAE\n",
    "        loss = nn.MSELoss()(outputs, inputs[\"targets\"])\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.05,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=0,  # since data is in memory, as problem is GPU bound\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=False,\n",
    ")\n",
    "\n",
    "# reinstantiate a clean model\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "trainer = RegressionTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69560' max='69560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69560/69560 31:07, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3478</td>\n",
       "      <td>1.650700</td>\n",
       "      <td>1.121598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6956</td>\n",
       "      <td>1.145400</td>\n",
       "      <td>1.094314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10434</td>\n",
       "      <td>1.119000</td>\n",
       "      <td>1.097998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13912</td>\n",
       "      <td>1.097600</td>\n",
       "      <td>1.075459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17390</td>\n",
       "      <td>1.075500</td>\n",
       "      <td>1.076842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20868</td>\n",
       "      <td>1.051700</td>\n",
       "      <td>1.054685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24346</td>\n",
       "      <td>1.030100</td>\n",
       "      <td>1.049676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27824</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.050678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31302</td>\n",
       "      <td>0.992800</td>\n",
       "      <td>1.045853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34780</td>\n",
       "      <td>0.974100</td>\n",
       "      <td>1.051777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38258</td>\n",
       "      <td>0.956300</td>\n",
       "      <td>1.053407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41736</td>\n",
       "      <td>0.940200</td>\n",
       "      <td>1.060211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45214</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>1.060131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48692</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>1.068415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52170</td>\n",
       "      <td>0.896800</td>\n",
       "      <td>1.075259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55648</td>\n",
       "      <td>0.887700</td>\n",
       "      <td>1.073634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59126</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>1.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62604</td>\n",
       "      <td>0.874800</td>\n",
       "      <td>1.079629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66082</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>1.079943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69560</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>1.080560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=69560, training_loss=1.0079467422459027, metrics={'train_runtime': 1870.1251, 'train_samples_per_second': 2380.076, 'train_steps_per_second': 37.195, 'total_flos': 0.0, 'train_loss': 1.0079467422459027, 'epoch': 20.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.6507,\n",
       "  'grad_norm': 19.227664947509766,\n",
       "  'learning_rate': 4.96934436859867e-05,\n",
       "  'epoch': 1.0,\n",
       "  'step': 3478},\n",
       " {'eval_loss': 1.1215980052947998,\n",
       "  'eval_runtime': 3.0562,\n",
       "  'eval_samples_per_second': 6544.165,\n",
       "  'eval_steps_per_second': 102.416,\n",
       "  'epoch': 1.0,\n",
       "  'step': 3478},\n",
       " {'loss': 1.1454,\n",
       "  'grad_norm': 10.553370475769043,\n",
       "  'learning_rate': 4.8778854084940764e-05,\n",
       "  'epoch': 2.0,\n",
       "  'step': 6956},\n",
       " {'eval_loss': 1.0943138599395752,\n",
       "  'eval_runtime': 2.728,\n",
       "  'eval_samples_per_second': 7331.389,\n",
       "  'eval_steps_per_second': 114.736,\n",
       "  'epoch': 2.0,\n",
       "  'step': 6956}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = trainer.state.log_history\n",
    "\n",
    "logs[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>loss</th><th>grad_norm</th><th>learning_rate</th><th>epoch</th><th>step</th><th>eval_loss</th><th>eval_runtime</th><th>eval_samples_per_second</th><th>eval_steps_per_second</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.6507</td><td>19.227665</td><td>0.00005</td><td>1.0</td><td>3478</td><td>1.121598</td><td>3.0562</td><td>6544.165</td><td>102.416</td></tr><tr><td>1.1454</td><td>10.55337</td><td>0.000049</td><td>2.0</td><td>6956</td><td>1.094314</td><td>2.728</td><td>7331.389</td><td>114.736</td></tr><tr><td>1.119</td><td>4.867875</td><td>0.000047</td><td>3.0</td><td>10434</td><td>1.097998</td><td>2.725</td><td>7339.382</td><td>114.861</td></tr><tr><td>1.0976</td><td>9.907719</td><td>0.000045</td><td>4.0</td><td>13912</td><td>1.075459</td><td>2.695</td><td>7421.251</td><td>116.143</td></tr><tr><td>1.0755</td><td>6.433779</td><td>0.000043</td><td>5.0</td><td>17390</td><td>1.076842</td><td>2.7045</td><td>7395.133</td><td>115.734</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.8877</td><td>15.479814</td><td>0.000005</td><td>16.0</td><td>55648</td><td>1.073634</td><td>2.7298</td><td>7326.416</td><td>114.658</td></tr><tr><td>0.8804</td><td>13.94219</td><td>0.000003</td><td>17.0</td><td>59126</td><td>1.073171</td><td>2.6996</td><td>7408.445</td><td>115.942</td></tr><tr><td>0.8748</td><td>13.542985</td><td>0.000001</td><td>18.0</td><td>62604</td><td>1.079629</td><td>2.7387</td><td>7302.754</td><td>114.288</td></tr><tr><td>0.8714</td><td>12.781465</td><td>3.1311e-7</td><td>19.0</td><td>66082</td><td>1.079943</td><td>2.6673</td><td>7498.317</td><td>117.349</td></tr><tr><td>0.8714</td><td>8.722484</td><td>2.4503e-11</td><td>20.0</td><td>69560</td><td>1.08056</td><td>2.7032</td><td>7398.709</td><td>115.79</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20, 9)\n",
       "┌────────┬───────────┬─────────────┬───────┬───┬───────────┬─────────────┬────────────┬────────────┐\n",
       "│ loss   ┆ grad_norm ┆ learning_ra ┆ epoch ┆ … ┆ eval_loss ┆ eval_runtim ┆ eval_sampl ┆ eval_steps │\n",
       "│ ---    ┆ ---       ┆ te          ┆ ---   ┆   ┆ ---       ┆ e           ┆ es_per_sec ┆ _per_secon │\n",
       "│ f64    ┆ f64       ┆ ---         ┆ f64   ┆   ┆ f64       ┆ ---         ┆ ond        ┆ d          │\n",
       "│        ┆           ┆ f64         ┆       ┆   ┆           ┆ f64         ┆ ---        ┆ ---        │\n",
       "│        ┆           ┆             ┆       ┆   ┆           ┆             ┆ f64        ┆ f64        │\n",
       "╞════════╪═══════════╪═════════════╪═══════╪═══╪═══════════╪═════════════╪════════════╪════════════╡\n",
       "│ 1.6507 ┆ 19.227665 ┆ 0.00005     ┆ 1.0   ┆ … ┆ 1.121598  ┆ 3.0562      ┆ 6544.165   ┆ 102.416    │\n",
       "│ 1.1454 ┆ 10.55337  ┆ 0.000049    ┆ 2.0   ┆ … ┆ 1.094314  ┆ 2.728       ┆ 7331.389   ┆ 114.736    │\n",
       "│ 1.119  ┆ 4.867875  ┆ 0.000047    ┆ 3.0   ┆ … ┆ 1.097998  ┆ 2.725       ┆ 7339.382   ┆ 114.861    │\n",
       "│ 1.0976 ┆ 9.907719  ┆ 0.000045    ┆ 4.0   ┆ … ┆ 1.075459  ┆ 2.695       ┆ 7421.251   ┆ 116.143    │\n",
       "│ 1.0755 ┆ 6.433779  ┆ 0.000043    ┆ 5.0   ┆ … ┆ 1.076842  ┆ 2.7045      ┆ 7395.133   ┆ 115.734    │\n",
       "│ …      ┆ …         ┆ …           ┆ …     ┆ … ┆ …         ┆ …           ┆ …          ┆ …          │\n",
       "│ 0.8877 ┆ 15.479814 ┆ 0.000005    ┆ 16.0  ┆ … ┆ 1.073634  ┆ 2.7298      ┆ 7326.416   ┆ 114.658    │\n",
       "│ 0.8804 ┆ 13.94219  ┆ 0.000003    ┆ 17.0  ┆ … ┆ 1.073171  ┆ 2.6996      ┆ 7408.445   ┆ 115.942    │\n",
       "│ 0.8748 ┆ 13.542985 ┆ 0.000001    ┆ 18.0  ┆ … ┆ 1.079629  ┆ 2.7387      ┆ 7302.754   ┆ 114.288    │\n",
       "│ 0.8714 ┆ 12.781465 ┆ 3.1311e-7   ┆ 19.0  ┆ … ┆ 1.079943  ┆ 2.6673      ┆ 7498.317   ┆ 117.349    │\n",
       "│ 0.8714 ┆ 8.722484  ┆ 2.4503e-11  ┆ 20.0  ┆ … ┆ 1.08056   ┆ 2.7032      ┆ 7398.709   ┆ 115.79     │\n",
       "└────────┴───────────┴─────────────┴───────┴───┴───────────┴─────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_consolidated = []\n",
    "i = 0\n",
    "while i < len(logs)-1:\n",
    "    base_log = logs[i]\n",
    "    base_log.update(logs[i+1])\n",
    "    logs_consolidated.append(base_log)\n",
    "    i += 2\n",
    "    \n",
    "df_logs = pl.DataFrame(logs_consolidated).sort(\"epoch\")\n",
    "df_logs.write_parquet(\"llm_scratch_train_logs.parquet\")\n",
    "df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_model\n",
    "\n",
    "save_model(model, \"imdb_embeddings_llm_scratch.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "\n",
    "In this case, need to evaluate the LLM in batches to avoid going OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:03<00:00, 103.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20_000, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Predicted</th><th>Actual</th><th>abs_diff</th><th>square_error</th></tr><tr><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>7.160156</td><td>7.1</td><td>0.060156</td><td>0.003619</td></tr><tr><td>6.417969</td><td>6.5</td><td>0.082031</td><td>0.006729</td></tr><tr><td>6.140625</td><td>4.1</td><td>2.040625</td><td>4.164151</td></tr><tr><td>6.042969</td><td>5.5</td><td>0.542969</td><td>0.294815</td></tr><tr><td>7.324219</td><td>7.2</td><td>0.124219</td><td>0.01543</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>6.484375</td><td>6.2</td><td>0.284375</td><td>0.080869</td></tr><tr><td>4.2421875</td><td>3.2</td><td>1.042187</td><td>1.086155</td></tr><tr><td>5.90625</td><td>5.7</td><td>0.20625</td><td>0.042539</td></tr><tr><td>6.34375</td><td>5.7</td><td>0.64375</td><td>0.414414</td></tr><tr><td>5.792969</td><td>5.8</td><td>0.007031</td><td>0.000049</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20_000, 4)\n",
       "┌───────────┬────────┬──────────┬──────────────┐\n",
       "│ Predicted ┆ Actual ┆ abs_diff ┆ square_error │\n",
       "│ ---       ┆ ---    ┆ ---      ┆ ---          │\n",
       "│ f32       ┆ f32    ┆ f32      ┆ f32          │\n",
       "╞═══════════╪════════╪══════════╪══════════════╡\n",
       "│ 7.160156  ┆ 7.1    ┆ 0.060156 ┆ 0.003619     │\n",
       "│ 6.417969  ┆ 6.5    ┆ 0.082031 ┆ 0.006729     │\n",
       "│ 6.140625  ┆ 4.1    ┆ 2.040625 ┆ 4.164151     │\n",
       "│ 6.042969  ┆ 5.5    ┆ 0.542969 ┆ 0.294815     │\n",
       "│ 7.324219  ┆ 7.2    ┆ 0.124219 ┆ 0.01543      │\n",
       "│ …         ┆ …      ┆ …        ┆ …            │\n",
       "│ 6.484375  ┆ 6.2    ┆ 0.284375 ┆ 0.080869     │\n",
       "│ 4.2421875 ┆ 3.2    ┆ 1.042187 ┆ 1.086155     │\n",
       "│ 5.90625   ┆ 5.7    ┆ 0.20625  ┆ 0.042539     │\n",
       "│ 6.34375   ┆ 5.7    ┆ 0.64375  ┆ 0.414414     │\n",
       "│ 5.792969  ┆ 5.8    ┆ 0.007031 ┆ 0.000049     │\n",
       "└───────────┴────────┴──────────┴──────────────┘"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = model.to(\"cuda:0\").eval()  # to disable BatchNorm1D\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64,\n",
    "                                         shuffle=False,\n",
    "                                         pin_memory=False)\n",
    "preds_bucket = []\n",
    "\n",
    "for batch in tqdm(dataloader, smoothing=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=batch[0],\n",
    "                       attention_mask=batch[1])\n",
    "        preds = output.detach().cpu().numpy()\n",
    "\n",
    "    preds_bucket.append(preds)\n",
    "        \n",
    "actual_values = torch.stack([f[2] for f in test_dataset])\n",
    "\n",
    "test_results = (pl.DataFrame({\"Predicted\": np.hstack(preds_bucket), \"Actual\": actual_values.cpu().numpy()})\n",
    "                .with_columns(\n",
    "                    abs_diff=(pl.col(\"Predicted\") - pl.col(\"Actual\")).abs(),\n",
    "                    square_error = ((pl.col(\"Actual\") - pl.col(\"Predicted\")) ** 2)\n",
    "                )\n",
    "               )\n",
    "                \n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7747781070888042"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "test_results[\"abs_diff\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0805600972121756"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Square Error (MSE)\n",
    "test_results[\"square_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu123.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu123:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
