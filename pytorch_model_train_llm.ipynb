{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Training\n",
    "\n",
    "Uses the Trainer included in Hugging Face `transformers` (backed by `accelerate`) since it mitigates a lot of annoying boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import Trainer, TrainingArguments, ModernBertConfig, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (242_552, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tconst</th><th>averageRating</th><th>json</th></tr><tr><td>str</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>&quot;tt0173052&quot;</td><td>4.1</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Prince and t…</td></tr><tr><td>&quot;tt0266288&quot;</td><td>7.4</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Azhakiya Ravanan…</td></tr><tr><td>&quot;tt6263490&quot;</td><td>4.3</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Getaway&quot;,\n",
       "&nbsp;&nbsp;&quot;gen…</td></tr><tr><td>&quot;tt10049110&quot;</td><td>7.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Die Wiese&quot;,\n",
       "&nbsp;&nbsp;&quot;g…</td></tr><tr><td>&quot;tt5761612&quot;</td><td>3.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Woman on the Edg…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;tt0079376&quot;</td><td>6.2</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;The Proud Twins&quot;…</td></tr><tr><td>&quot;tt1161064&quot;</td><td>3.2</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Super Capers: Th…</td></tr><tr><td>&quot;tt0179526&quot;</td><td>5.7</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Who&#x27;s the Caboos…</td></tr><tr><td>&quot;tt0188233&quot;</td><td>5.7</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;That&#x27;s Erotic&quot;,\n",
       "…</td></tr><tr><td>&quot;tt0082518&quot;</td><td>5.8</td><td>&quot;{\n",
       "&nbsp;&nbsp;&quot;title&quot;: &quot;Hoge hakken, ech…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (242_552, 3)\n",
       "┌────────────┬───────────────┬───────────────────────────────┐\n",
       "│ tconst     ┆ averageRating ┆ json                          │\n",
       "│ ---        ┆ ---           ┆ ---                           │\n",
       "│ str        ┆ f32           ┆ str                           │\n",
       "╞════════════╪═══════════════╪═══════════════════════════════╡\n",
       "│ tt0173052  ┆ 4.1           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"The Prince and t… │\n",
       "│ tt0266288  ┆ 7.4           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Azhakiya Ravanan… │\n",
       "│ tt6263490  ┆ 4.3           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Getaway\",         │\n",
       "│            ┆               ┆   \"gen…                       │\n",
       "│ tt10049110 ┆ 7.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Die Wiese\",       │\n",
       "│            ┆               ┆   \"g…                         │\n",
       "│ tt5761612  ┆ 3.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Woman on the Edg… │\n",
       "│ …          ┆ …             ┆ …                             │\n",
       "│ tt0079376  ┆ 6.2           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"The Proud Twins\"… │\n",
       "│ tt1161064  ┆ 3.2           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Super Capers: Th… │\n",
       "│ tt0179526  ┆ 5.7           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Who's the Caboos… │\n",
       "│ tt0188233  ┆ 5.7           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"That's Erotic\",   │\n",
       "│            ┆               ┆ …                             │\n",
       "│ tt0082518  ┆ 5.8           ┆ {                             │\n",
       "│            ┆               ┆   \"title\": \"Hoge hakken, ech… │\n",
       "└────────────┴───────────────┴───────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pl.scan_parquet(\n",
    "        \"movie_data_plus_embeds_all.parquet\",\n",
    "    )\n",
    "    .select([\"tconst\", \"averageRating\", \"json\"])\n",
    "    .with_columns(averageRating=pl.col(\"averageRating\").cast(pl.Float32))\n",
    "    .collect()\n",
    "    .sample(fraction=1.0, shuffle=True, seed=42)\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Custom Tokenizer\n",
    "\n",
    "Use the `modernbert` tokenizer as a base, just reduce the vocabulary significantly and tailor it to this specific dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ĠprÃ©': 19856, 'Ġdisclaim': 23464, 'illon': 24632, 'ĠAB': 12056, 'Ġreactor': 22578, 'Ġsalvage': 40\n",
      "378\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "json_docs = df[\"json\"].to_list()\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "print(str(base_tokenizer.vocab)[0:100])\n",
    "print(len(base_tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{'Mass': 2476, ',': 18, 'ĠMcDonald': 4206, 'ĠCla': 4298, 'str': 1686, 'Lor': 4188, '¦': 106, 'ĠLip':\n",
      "302\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "# don't train on all texts because it will take forever\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    iter(json_docs[:50000]), vocab_size=vocab_size,\n",
    "    # new_special_tokens=[\"  \", \"    \", \"      \"]\n",
    ")\n",
    "\n",
    "print(str(tokenizer.vocab)[0:100])\n",
    "print(len(tokenizer(json_docs[0])[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preencode all the tokens. A `max_length` of 1024 may be excessive but does not cause a proportionate reduction in model training speed over a 512 max length due to ModernBERT's unpadding + RoPE.\n",
    "\n",
    "In order to avoid OOMs on the host system, generate in batches, then push to the GPU. (ideally we _could_ push to the GPU for each batch but that will cause GPU memory leaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [03:22,  1.70s/it]                         \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([242552, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 1024\n",
    "token_batch_size = 2048\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# input_ids = torch.empty((0, max_length)).to(\"cpu\")\n",
    "# attention_mask = torch.empty((0, max_length)).to(\"cpu\")\n",
    "\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for docs in tqdm(batch(json_docs, token_batch_size), total=len(json_docs) // token_batch_size):\n",
    "    tokens = tokenizer(docs,\n",
    "                       max_length=max_length,\n",
    "                       padding=\"max_length\",\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\").to(\"cpu\")\n",
    "    \n",
    "    # input_ids = torch.vstack([input_ids, tokens[\"input_ids\"]])\n",
    "    # attention_mask = torch.vstack([attention_mask, tokens[\"attention_mask\"]])\n",
    "    \n",
    "    input_ids.append(tokens[\"input_ids\"])\n",
    "    attention_mask.append(tokens[\"attention_mask\"])\n",
    "   \n",
    "input_ids = torch.vstack(input_ids).to(device)\n",
    "attention_mask = torch.vstack(attention_mask).to(device)\n",
    "\n",
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1000, 7.4000, 4.3000,  ..., 6.4000, 6.0000, 6.5000], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "n_test = 20000\n",
    "\n",
    "X_input_ids_train = input_ids[:-n_test].int().to(device)\n",
    "X_input_ids_test = input_ids[-n_test:].int().to(device)\n",
    "\n",
    "X_attention_train = attention_mask[:-n_test].int().to(device)\n",
    "X_attention_test = attention_mask[-n_test:].int().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(df[:-n_test][\"averageRating\"].to_numpy().copy()).to(device)\n",
    "y_test = torch.from_numpy(df[-n_test:][\"averageRating\"].to_numpy().copy()).to(device)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_input_ids_train, X_attention_train, y_train)\n",
    "test_dataset = TensorDataset(X_input_ids_test, X_attention_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Due to the new tokenizer, the special tokens for the fresh ModernBERT model have to be explicitly defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': 2,\n",
       " 'sep_token': 4,\n",
       " 'pad_token': 5,\n",
       " 'cls_token': 3,\n",
       " 'mask_token': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token_dict = dict(\n",
    "    zip(tokenizer.special_tokens_map.keys(), tokenizer.all_special_ids)\n",
    ")\n",
    "special_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2214528"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "dropout = 0.5\n",
    "\n",
    "config = ModernBertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_length,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=512,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    global_attn_every_n_layers=2,\n",
    "    local_attention=32,\n",
    "    attention_dropout=dropout,\n",
    "    embeddings_dropout=dropout,\n",
    "    mlp_dropout=dropout,\n",
    "    unk_token_id=special_token_dict[\"unk_token\"],\n",
    "    sep_token_id=special_token_dict[\"sep_token\"],\n",
    "    pad_token_id=special_token_dict[\"pad_token\"],\n",
    "    cls_token_id=special_token_dict[\"cls_token\"],\n",
    "    mask_token_id=special_token_dict[\"mask_token\"],\n",
    ")\n",
    "\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RatingsModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.transformer_model = model\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, targets=None):\n",
    "        x = self.transformer_model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        x = x.last_hidden_state[:, 0]  # the [CLS] vector\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x.squeeze()  # return 1D output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # perf increase for ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss doesn't play nice with the `Trainer` out of the boss, so need [some tweaks](https://discuss.huggingface.co/t/no-log-for-validation-loss-during-training-with-trainer/40094/3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids = torch.stack([f[0] for f in examples])\n",
    "    attention_masks = torch.stack([f[1] for f in examples])\n",
    "    targets = torch.stack([f[2] for f in examples])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"targets\": targets,\n",
    "    }\n",
    "\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=0):\n",
    "        outputs = model(**inputs)\n",
    "        # loss = nn.L1Loss()(outputs, inputs[\"targets\"])  # L1 loss is MAE\n",
    "        loss = nn.MSELoss()(outputs, inputs[\"targets\"])\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.001,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.05,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=0,  # since data is in memory, as problem is GPU bound\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=False,\n",
    ")\n",
    "\n",
    "# reinstantiate a clean model\n",
    "transformer_model = AutoModel.from_config(config)\n",
    "model = RatingsModel(transformer_model)\n",
    "_ = model.to(device)\n",
    "\n",
    "trainer = RegressionTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69550' max='69550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69550/69550 28:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3478</td>\n",
       "      <td>1.751500</td>\n",
       "      <td>1.127283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6956</td>\n",
       "      <td>1.182300</td>\n",
       "      <td>1.100623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10434</td>\n",
       "      <td>1.115600</td>\n",
       "      <td>1.079834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13912</td>\n",
       "      <td>1.117700</td>\n",
       "      <td>1.066914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17390</td>\n",
       "      <td>1.079800</td>\n",
       "      <td>1.065529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20868</td>\n",
       "      <td>1.073800</td>\n",
       "      <td>1.065454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24346</td>\n",
       "      <td>1.034700</td>\n",
       "      <td>1.061610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27824</td>\n",
       "      <td>1.045300</td>\n",
       "      <td>1.048201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31302</td>\n",
       "      <td>1.002100</td>\n",
       "      <td>1.053628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34780</td>\n",
       "      <td>1.012400</td>\n",
       "      <td>1.048743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38258</td>\n",
       "      <td>0.983700</td>\n",
       "      <td>1.059218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41736</td>\n",
       "      <td>0.972600</td>\n",
       "      <td>1.044247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45214</td>\n",
       "      <td>0.956400</td>\n",
       "      <td>1.050604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48692</td>\n",
       "      <td>0.946800</td>\n",
       "      <td>1.053110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52170</td>\n",
       "      <td>0.931700</td>\n",
       "      <td>1.058548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55648</td>\n",
       "      <td>0.932800</td>\n",
       "      <td>1.055869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59126</td>\n",
       "      <td>0.921700</td>\n",
       "      <td>1.066425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62604</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>1.059349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66082</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>1.061114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=69550, training_loss=1.0403015373663282, metrics={'train_runtime': 1726.1904, 'train_samples_per_second': 1289.267, 'train_steps_per_second': 40.291, 'total_flos': 0.0, 'train_loss': 1.0403015373663282, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.7515,\n",
       "  'grad_norm': 4.927058219909668,\n",
       "  'learning_rate': 4.969317935368992e-05,\n",
       "  'epoch': 0.5000718907260964,\n",
       "  'step': 3478},\n",
       " {'eval_loss': 1.1272826194763184,\n",
       "  'eval_runtime': 5.9426,\n",
       "  'eval_samples_per_second': 3365.521,\n",
       "  'eval_steps_per_second': 105.173,\n",
       "  'epoch': 0.5000718907260964,\n",
       "  'step': 3478},\n",
       " {'loss': 1.1823,\n",
       "  'grad_norm': 4.919924259185791,\n",
       "  'learning_rate': 4.8778157098802465e-05,\n",
       "  'epoch': 1.0001437814521927,\n",
       "  'step': 6956},\n",
       " {'eval_loss': 1.1006231307983398,\n",
       "  'eval_runtime': 5.4397,\n",
       "  'eval_samples_per_second': 3676.699,\n",
       "  'eval_steps_per_second': 114.897,\n",
       "  'epoch': 1.0001437814521927,\n",
       "  'step': 6956}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = trainer.state.log_history\n",
    "\n",
    "logs[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (19, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>loss</th><th>grad_norm</th><th>learning_rate</th><th>epoch</th><th>step</th><th>eval_loss</th><th>eval_runtime</th><th>eval_samples_per_second</th><th>eval_steps_per_second</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.7515</td><td>4.927058</td><td>0.00005</td><td>0.500072</td><td>3478</td><td>1.127283</td><td>5.9426</td><td>3365.521</td><td>105.173</td></tr><tr><td>1.1823</td><td>4.919924</td><td>0.000049</td><td>1.000144</td><td>6956</td><td>1.100623</td><td>5.4397</td><td>3676.699</td><td>114.897</td></tr><tr><td>1.1156</td><td>6.001026</td><td>0.000047</td><td>1.500216</td><td>10434</td><td>1.079834</td><td>5.3925</td><td>3708.867</td><td>115.902</td></tr><tr><td>1.1177</td><td>6.690367</td><td>0.000045</td><td>2.000288</td><td>13912</td><td>1.066914</td><td>5.4362</td><td>3679.061</td><td>114.971</td></tr><tr><td>1.0798</td><td>9.009649</td><td>0.000043</td><td>2.500359</td><td>17390</td><td>1.065529</td><td>5.4792</td><td>3650.18</td><td>114.068</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.9317</td><td>9.880666</td><td>0.000007</td><td>7.501078</td><td>52170</td><td>1.058548</td><td>5.3844</td><td>3714.437</td><td>116.076</td></tr><tr><td>0.9328</td><td>8.97761</td><td>0.000005</td><td>8.00115</td><td>55648</td><td>1.055869</td><td>5.4355</td><td>3679.487</td><td>114.984</td></tr><tr><td>0.9217</td><td>6.215415</td><td>0.000003</td><td>8.501222</td><td>59126</td><td>1.066425</td><td>5.4003</td><td>3703.501</td><td>115.734</td></tr><tr><td>0.9177</td><td>4.816526</td><td>0.000001</td><td>9.001294</td><td>62604</td><td>1.059349</td><td>5.3999</td><td>3703.802</td><td>115.744</td></tr><tr><td>0.9147</td><td>4.794697</td><td>3.1213e-7</td><td>9.501366</td><td>66082</td><td>1.061114</td><td>5.3823</td><td>3715.898</td><td>116.122</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (19, 9)\n",
       "┌────────┬───────────┬────────────┬──────────┬───┬───────────┬────────────┬────────────┬───────────┐\n",
       "│ loss   ┆ grad_norm ┆ learning_r ┆ epoch    ┆ … ┆ eval_loss ┆ eval_runti ┆ eval_sampl ┆ eval_step │\n",
       "│ ---    ┆ ---       ┆ ate        ┆ ---      ┆   ┆ ---       ┆ me         ┆ es_per_sec ┆ s_per_sec │\n",
       "│ f64    ┆ f64       ┆ ---        ┆ f64      ┆   ┆ f64       ┆ ---        ┆ ond        ┆ ond       │\n",
       "│        ┆           ┆ f64        ┆          ┆   ┆           ┆ f64        ┆ ---        ┆ ---       │\n",
       "│        ┆           ┆            ┆          ┆   ┆           ┆            ┆ f64        ┆ f64       │\n",
       "╞════════╪═══════════╪════════════╪══════════╪═══╪═══════════╪════════════╪════════════╪═══════════╡\n",
       "│ 1.7515 ┆ 4.927058  ┆ 0.00005    ┆ 0.500072 ┆ … ┆ 1.127283  ┆ 5.9426     ┆ 3365.521   ┆ 105.173   │\n",
       "│ 1.1823 ┆ 4.919924  ┆ 0.000049   ┆ 1.000144 ┆ … ┆ 1.100623  ┆ 5.4397     ┆ 3676.699   ┆ 114.897   │\n",
       "│ 1.1156 ┆ 6.001026  ┆ 0.000047   ┆ 1.500216 ┆ … ┆ 1.079834  ┆ 5.3925     ┆ 3708.867   ┆ 115.902   │\n",
       "│ 1.1177 ┆ 6.690367  ┆ 0.000045   ┆ 2.000288 ┆ … ┆ 1.066914  ┆ 5.4362     ┆ 3679.061   ┆ 114.971   │\n",
       "│ 1.0798 ┆ 9.009649  ┆ 0.000043   ┆ 2.500359 ┆ … ┆ 1.065529  ┆ 5.4792     ┆ 3650.18    ┆ 114.068   │\n",
       "│ …      ┆ …         ┆ …          ┆ …        ┆ … ┆ …         ┆ …          ┆ …          ┆ …         │\n",
       "│ 0.9317 ┆ 9.880666  ┆ 0.000007   ┆ 7.501078 ┆ … ┆ 1.058548  ┆ 5.3844     ┆ 3714.437   ┆ 116.076   │\n",
       "│ 0.9328 ┆ 8.97761   ┆ 0.000005   ┆ 8.00115  ┆ … ┆ 1.055869  ┆ 5.4355     ┆ 3679.487   ┆ 114.984   │\n",
       "│ 0.9217 ┆ 6.215415  ┆ 0.000003   ┆ 8.501222 ┆ … ┆ 1.066425  ┆ 5.4003     ┆ 3703.501   ┆ 115.734   │\n",
       "│ 0.9177 ┆ 4.816526  ┆ 0.000001   ┆ 9.001294 ┆ … ┆ 1.059349  ┆ 5.3999     ┆ 3703.802   ┆ 115.744   │\n",
       "│ 0.9147 ┆ 4.794697  ┆ 3.1213e-7  ┆ 9.501366 ┆ … ┆ 1.061114  ┆ 5.3823     ┆ 3715.898   ┆ 116.122   │\n",
       "└────────┴───────────┴────────────┴──────────┴───┴───────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_consolidated = []\n",
    "i = 0\n",
    "while i < len(logs)-1:\n",
    "    base_log = logs[i]\n",
    "    base_log.update(logs[i+1])\n",
    "    logs_consolidated.append(base_log)\n",
    "    i += 2\n",
    "    \n",
    "df_logs = pl.DataFrame(logs_consolidated).sort(\"epoch\")\n",
    "df_logs.write_parquet(\"llm_scratch_train_logs.parquet\")\n",
    "df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_model\n",
    "\n",
    "save_model(model, \"imdb_embeddings_llm_scratch.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "\n",
    "In this case, need to evaluate the LLM in batches to avoid going OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:03<00:00, 100.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20_000, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Predicted</th><th>Actual</th><th>abs_diff</th><th>square_error</th></tr><tr><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>7.191406</td><td>7.1</td><td>0.091406</td><td>0.008355</td></tr><tr><td>6.472656</td><td>6.5</td><td>0.027344</td><td>0.000748</td></tr><tr><td>5.234375</td><td>4.1</td><td>1.134375</td><td>1.286807</td></tr><tr><td>6.011719</td><td>5.5</td><td>0.511719</td><td>0.261856</td></tr><tr><td>7.683594</td><td>7.2</td><td>0.483594</td><td>0.233863</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>6.421875</td><td>6.2</td><td>0.221875</td><td>0.049229</td></tr><tr><td>4.675781</td><td>3.2</td><td>1.475781</td><td>2.17793</td></tr><tr><td>5.636719</td><td>5.7</td><td>0.063281</td><td>0.004004</td></tr><tr><td>5.65625</td><td>5.7</td><td>0.04375</td><td>0.001914</td></tr><tr><td>5.316406</td><td>5.8</td><td>0.483594</td><td>0.233863</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20_000, 4)\n",
       "┌───────────┬────────┬──────────┬──────────────┐\n",
       "│ Predicted ┆ Actual ┆ abs_diff ┆ square_error │\n",
       "│ ---       ┆ ---    ┆ ---      ┆ ---          │\n",
       "│ f32       ┆ f32    ┆ f32      ┆ f32          │\n",
       "╞═══════════╪════════╪══════════╪══════════════╡\n",
       "│ 7.191406  ┆ 7.1    ┆ 0.091406 ┆ 0.008355     │\n",
       "│ 6.472656  ┆ 6.5    ┆ 0.027344 ┆ 0.000748     │\n",
       "│ 5.234375  ┆ 4.1    ┆ 1.134375 ┆ 1.286807     │\n",
       "│ 6.011719  ┆ 5.5    ┆ 0.511719 ┆ 0.261856     │\n",
       "│ 7.683594  ┆ 7.2    ┆ 0.483594 ┆ 0.233863     │\n",
       "│ …         ┆ …      ┆ …        ┆ …            │\n",
       "│ 6.421875  ┆ 6.2    ┆ 0.221875 ┆ 0.049229     │\n",
       "│ 4.675781  ┆ 3.2    ┆ 1.475781 ┆ 2.17793      │\n",
       "│ 5.636719  ┆ 5.7    ┆ 0.063281 ┆ 0.004004     │\n",
       "│ 5.65625   ┆ 5.7    ┆ 0.04375  ┆ 0.001914     │\n",
       "│ 5.316406  ┆ 5.8    ┆ 0.483594 ┆ 0.233863     │\n",
       "└───────────┴────────┴──────────┴──────────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = model.to(\"cuda:0\").eval()  # to disable BatchNorm1D\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64,\n",
    "                                         shuffle=False,\n",
    "                                         pin_memory=False)\n",
    "preds_bucket = []\n",
    "\n",
    "for batch in tqdm(dataloader, smoothing=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=batch[0],\n",
    "                       attention_mask=batch[1])\n",
    "        preds = output.detach().cpu().numpy()\n",
    "\n",
    "    preds_bucket.append(preds)\n",
    "        \n",
    "actual_values = torch.stack([f[2] for f in test_dataset])\n",
    "\n",
    "test_results = (pl.DataFrame({\"Predicted\": np.hstack(preds_bucket), \"Actual\": actual_values.cpu().numpy()})\n",
    "                .with_columns(\n",
    "                    abs_diff=(pl.col(\"Predicted\") - pl.col(\"Actual\")).abs(),\n",
    "                    square_error = ((pl.col(\"Actual\") - pl.col(\"Predicted\")) ** 2)\n",
    "                )\n",
    "               )\n",
    "                \n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7710315263330937"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "test_results[\"abs_diff\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0620494208003317"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Square Error (MSE)\n",
    "test_results[\"square_error\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu123.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu123:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
